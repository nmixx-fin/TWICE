{
  "dataset_revision": "main",
  "evaluation_time": 550.230959892273,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "train": [
      {
        "accuracy": 0.5253511450381679,
        "ap": 0.6926555618837077,
        "ap_weighted": 0.6926555618837077,
        "f1": 0.5008838615191314,
        "f1_weighted": 0.5374079975209631,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.5253511450381679,
        "scores_per_experiment": [
          {
            "accuracy": 0.5174045801526718,
            "ap": 0.703894984336531,
            "ap_weighted": 0.703894984336531,
            "f1": 0.510133466156573,
            "f1_weighted": 0.5319468081448695
          },
          {
            "accuracy": 0.4983969465648855,
            "ap": 0.682602288754855,
            "ap_weighted": 0.682602288754855,
            "f1": 0.4817623390494797,
            "f1_weighted": 0.515697788893435
          },
          {
            "accuracy": 0.5599236641221375,
            "ap": 0.6937802883750486,
            "ap_weighted": 0.6937802883750486,
            "f1": 0.5201619441604763,
            "f1_weighted": 0.5706469919399602
          },
          {
            "accuracy": 0.4833587786259542,
            "ap": 0.6807733174711093,
            "ap_weighted": 0.6807733174711093,
            "f1": 0.4717741935483871,
            "f1_weighted": 0.5003654272346714
          },
          {
            "accuracy": 0.4783969465648855,
            "ap": 0.6905587232945433,
            "ap_weighted": 0.6905587232945433,
            "f1": 0.4748606481939815,
            "f1_weighted": 0.49061115665899374
          },
          {
            "accuracy": 0.5390839694656488,
            "ap": 0.6746001747059339,
            "ap_weighted": 0.6746001747059339,
            "f1": 0.48077398992634135,
            "f1_weighted": 0.5443703867450667
          },
          {
            "accuracy": 0.5067175572519084,
            "ap": 0.7018539233415853,
            "ap_weighted": 0.7018539233415853,
            "f1": 0.501454325651489,
            "f1_weighted": 0.5201767275109306
          },
          {
            "accuracy": 0.5612977099236641,
            "ap": 0.6964928952504176,
            "ap_weighted": 0.6964928952504176,
            "f1": 0.524658166988442,
            "f1_weighted": 0.5728930120733592
          },
          {
            "accuracy": 0.5377862595419848,
            "ap": 0.7128291071825836,
            "ap_weighted": 0.7128291071825836,
            "f1": 0.5292010971064752,
            "f1_weighted": 0.5524378171410821
          },
          {
            "accuracy": 0.5711450381679389,
            "ap": 0.6891699161244697,
            "ap_weighted": 0.6891699161244697,
            "f1": 0.5140584444096695,
            "f1_weighted": 0.5749338588672629
          }
        ]
      }
    ]
  },
  "task_name": "KorNewsBQAClassification"
}