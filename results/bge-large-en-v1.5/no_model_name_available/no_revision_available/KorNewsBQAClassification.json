{
  "dataset_revision": "main",
  "evaluation_time": 45.43332242965698,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "train": [
      {
        "accuracy": 0.49929007633587785,
        "ap": 0.684700576127738,
        "ap_weighted": 0.684700576127738,
        "f1": 0.4748243832299216,
        "f1_weighted": 0.5070178085086761,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.49929007633587785,
        "scores_per_experiment": [
          {
            "accuracy": 0.45946564885496183,
            "ap": 0.6965628901284392,
            "ap_weighted": 0.6965628901284392,
            "f1": 0.4594654945155249,
            "f1_weighted": 0.45957106269037384
          },
          {
            "accuracy": 0.47083969465648856,
            "ap": 0.6854644728475551,
            "ap_weighted": 0.6854644728475551,
            "f1": 0.46668675667894244,
            "f1_weighted": 0.4838876797208896
          },
          {
            "accuracy": 0.5636641221374046,
            "ap": 0.6871408033817896,
            "ap_weighted": 0.6871408033817896,
            "f1": 0.5091970462733175,
            "f1_weighted": 0.5689560653469309
          },
          {
            "accuracy": 0.5124427480916031,
            "ap": 0.6760183441445395,
            "ap_weighted": 0.6760183441445395,
            "f1": 0.4787403518259189,
            "f1_weighted": 0.5271843846449209
          },
          {
            "accuracy": 0.49916030534351147,
            "ap": 0.6766720834592983,
            "ap_weighted": 0.6766720834592983,
            "f1": 0.47505829250120524,
            "f1_weighted": 0.5161699553045406
          },
          {
            "accuracy": 0.5772519083969465,
            "ap": 0.6747681442199239,
            "ap_weighted": 0.6747681442199239,
            "f1": 0.47662926966597563,
            "f1_weighted": 0.5605048257669313
          },
          {
            "accuracy": 0.4851145038167939,
            "ap": 0.6860974148099217,
            "ap_weighted": 0.6860974148099217,
            "f1": 0.47708806306434126,
            "f1_weighted": 0.500766805099303
          },
          {
            "accuracy": 0.4319083969465649,
            "ap": 0.6814650504439825,
            "ap_weighted": 0.6814650504439825,
            "f1": 0.4319013921295131,
            "f1_weighted": 0.4326305022174232
          },
          {
            "accuracy": 0.4734351145038168,
            "ap": 0.6904354215676018,
            "ap_weighted": 0.6904354215676018,
            "f1": 0.4709043718956315,
            "f1_weighted": 0.484278759983922
          },
          {
            "accuracy": 0.519618320610687,
            "ap": 0.6923811362743283,
            "ap_weighted": 0.6923811362743283,
            "f1": 0.5025727937488451,
            "f1_weighted": 0.5362280443115252
          }
        ]
      }
    ]
  },
  "task_name": "KorNewsBQAClassification"
}