{
  "dataset_revision": "main",
  "evaluation_time": 28.90042281150818,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "train": [
      {
        "accuracy": 0.48851908396946575,
        "ap": 0.6849815898565089,
        "ap_weighted": 0.6849815898565089,
        "f1": 0.47530419138950053,
        "f1_weighted": 0.5016666117048625,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.48851908396946575,
        "scores_per_experiment": [
          {
            "accuracy": 0.496030534351145,
            "ap": 0.6883369356256287,
            "ap_weighted": 0.6883369356256287,
            "f1": 0.4856843899917932,
            "f1_weighted": 0.5123460364894125
          },
          {
            "accuracy": 0.4384732824427481,
            "ap": 0.6824802648457642,
            "ap_weighted": 0.6824802648457642,
            "f1": 0.43833168219741614,
            "f1_weighted": 0.44159121092169135
          },
          {
            "accuracy": 0.518320610687023,
            "ap": 0.6873341016809804,
            "ap_weighted": 0.6873341016809804,
            "f1": 0.49651881017322297,
            "f1_weighted": 0.5348119212737638
          },
          {
            "accuracy": 0.536793893129771,
            "ap": 0.6775226499557299,
            "ap_weighted": 0.6775226499557299,
            "f1": 0.4867108457236089,
            "f1_weighted": 0.5453124173220214
          },
          {
            "accuracy": 0.483206106870229,
            "ap": 0.6833362164642729,
            "ap_weighted": 0.6833362164642729,
            "f1": 0.47381598659131696,
            "f1_weighted": 0.4995073556744203
          },
          {
            "accuracy": 0.48351145038167936,
            "ap": 0.6826507850887924,
            "ap_weighted": 0.6826507850887924,
            "f1": 0.47345959429443507,
            "f1_weighted": 0.500049808076604
          },
          {
            "accuracy": 0.47480916030534354,
            "ap": 0.6875597640544624,
            "ap_weighted": 0.6875597640544624,
            "f1": 0.47071607238815105,
            "f1_weighted": 0.4877279690439822
          },
          {
            "accuracy": 0.4949618320610687,
            "ap": 0.6877559613888289,
            "ap_weighted": 0.6877559613888289,
            "f1": 0.48457095552552953,
            "f1_weighted": 0.5113190828654013
          },
          {
            "accuracy": 0.49893129770992367,
            "ap": 0.6847083182384539,
            "ap_weighted": 0.6847083182384539,
            "f1": 0.48417073554890533,
            "f1_weighted": 0.5160631415177481
          },
          {
            "accuracy": 0.4601526717557252,
            "ap": 0.6881309012221749,
            "ap_weighted": 0.6881309012221749,
            "f1": 0.4590628414606256,
            "f1_weighted": 0.4679371738635793
          }
        ]
      }
    ]
  },
  "task_name": "KorNewsBQAClassification"
}