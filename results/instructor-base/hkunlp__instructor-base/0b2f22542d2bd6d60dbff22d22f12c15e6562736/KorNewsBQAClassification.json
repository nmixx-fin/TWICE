{
  "dataset_revision": "main",
  "evaluation_time": 53.48999571800232,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "train": [
      {
        "accuracy": 0.500381679389313,
        "ap": 0.6848342154071685,
        "ap_weighted": 0.6848342154071685,
        "f1": 0.47625261274524966,
        "f1_weighted": 0.5080589714305408,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ],
        "main_score": 0.500381679389313,
        "scores_per_experiment": [
          {
            "accuracy": 0.4083206106870229,
            "ap": 0.689493674857462,
            "ap_weighted": 0.689493674857462,
            "f1": 0.4019218787803019,
            "f1_weighted": 0.37931145193943083
          },
          {
            "accuracy": 0.43809160305343514,
            "ap": 0.6844362849537242,
            "ap_weighted": 0.6844362849537242,
            "f1": 0.43808169801955893,
            "f1_weighted": 0.4389439762413614
          },
          {
            "accuracy": 0.5469465648854962,
            "ap": 0.6840865473551311,
            "ap_weighted": 0.6840865473551311,
            "f1": 0.5008494999208374,
            "f1_weighted": 0.5562910339702687
          },
          {
            "accuracy": 0.5421374045801527,
            "ap": 0.6757505216966715,
            "ap_weighted": 0.6757505216966715,
            "f1": 0.4834973112263783,
            "f1_weighted": 0.5471060033373596
          },
          {
            "accuracy": 0.47763358778625953,
            "ap": 0.6789361921214692,
            "ap_weighted": 0.6789361921214692,
            "f1": 0.4667797673735705,
            "f1_weighted": 0.49458506011618975
          },
          {
            "accuracy": 0.5027480916030534,
            "ap": 0.6795202423782851,
            "ap_weighted": 0.6795202423782851,
            "f1": 0.4802414468254159,
            "f1_weighted": 0.5197725602499678
          },
          {
            "accuracy": 0.5454961832061068,
            "ap": 0.6831203301407149,
            "ap_weighted": 0.6831203301407149,
            "f1": 0.4988150951335919,
            "f1_weighted": 0.5547203101639074
          },
          {
            "accuracy": 0.5234351145038167,
            "ap": 0.6946733716279918,
            "ap_weighted": 0.6946733716279918,
            "f1": 0.5067838635527566,
            "f1_weighted": 0.5399065015060911
          },
          {
            "accuracy": 0.49358778625954197,
            "ap": 0.6909722129727296,
            "ap_weighted": 0.6909722129727296,
            "f1": 0.486015948484137,
            "f1_weighted": 0.5088171807285642
          },
          {
            "accuracy": 0.5254198473282443,
            "ap": 0.6873527759675053,
            "ap_weighted": 0.6873527759675053,
            "f1": 0.4995396181359478,
            "f1_weighted": 0.5411356360522672
          }
        ]
      }
    ]
  },
  "task_name": "KorNewsBQAClassification"
}